<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Remote Proxmox Server</title>
    <link rel="stylesheet" href="../styles.css">

</head>

<body>
    <nav class="nav-bar">
        <div class="nav-container">
            <a href="../index.html" class="nav-logo">Portfolio</a>
            <ul class="nav-links">
                <li><a href="../index.html">Home</a></li>
                <li><a href="../projects.html" class="active">Projects</a></li>
                <!--     <li><a href="../../resume.html">Resume</a></li> -->
            </ul>
        </div>
    </nav>

    <main>
        <section class="projects-header">
            <h1>Remote Proxmox Server</h1>
            <p>A low-cost virtualization platform built for reliability, remote management, and real-world operational
                constraints.</p>
        </section>

        <section class="project-details">
            <div class="section-card">
                <h2>Overview</h2>
                <p>This project began as an attempt to reduce friction in my daily lab workflow.</p>
                <br>
                <p>Running a lab on my primary desktop created constant tradeoffs. Spinning up multiple VMs required
                    shutting down other applications, restarting VMs between sessions discouraged long-running projects,
                    and keeping a high-performance desktop powered on 24/7 introduced heat, noise, and power concerns
                    that were impractical in a dorm environment.</p>
                <br>
                <p>I set out to build a dedicated, always-on virtualization server that could host labs continuously,
                    survive power events, and be fully managed remotely without physical access.</p>
                <h3>Core Objectives</h3>
                <ul>
                    <li>Reliable 24/7 VM hosting</li>
                    <li>Strong data protection and recovery</li>
                    <li>Remote out-of-band management</li>
                    <li>Cost efficiency using repurposed hardware where possible</li>
                </ul>
            </div>
            <div class="section-card">
                <h2>System Design and Constraints</h2>
                <p>Rather than purchasing used enterprise hardware, I focused on maximizing performance per dollar while
                    implementing enterprise-like reliability and management features.</p>
                <ul>
                    <li><strong>CPU: </strong>AMD Ryzen 9 3950X. High core count for virtualization with acceptable
                        power consumption and heat output. Required no additional investment, as it was already obtained
                        for a prior project.</li>
                    <li><strong>RAM: </strong>64 GB ECC DDR4. Chosen explicitly to reduce silent data corruption risk
                        with ZFS.</li>
                    <li><strong>Storage: </strong>
                        <ul>
                            <li>2x NVMe SSDs for boot drives. One for Proxmox VE, one for Proxmox Backup Server</li>
                            <li>3x 1 TB NAS-rated SATA SSDs in RAIDZ1 for VM storage</li>
                            <li>2x 4TB HDDs in ZFS mirror dedicated to backup storage</li>
                        </ul>
                    </li>
                    <li><strong>UPS: </strong>APC Back-UPS series for affordability and strong community support.</li>
                    <li><strong>Out-of-Band Management: </strong>JetKVM and Tailscale for remote access and
                        troubleshooting.</li>
                </ul>
            </div>
            <div class="section-card">
                <h2>Architecture</h2>
                <img src="../assets/projects/proxmoxserver/diagram.png" alt="System Architecture Diagram"
                    style="max-width: 100%; height: auto; display: block; margin: 20px auto;">
            </div>
            <div class="section-card">
                <h2>Platform Selection and Early Friction</h2>
                <p>This project was originally scoped around Hyper-V Server.</p>
                <br>
                <p>In practice, running Hyper-V in a workgroup environment introduced persistent friction. Windows Admin
                    Center required domain services for meaningful management and MMC required unreliable WinRM and
                    CredSSP workarounds. That proved to be a chore for macOS clients and inconvenient for off-site
                    management.</p>
                <br>
                <p>Proxmox VE offered:</p>
                <ul>
                    <li>Platform-agnostic management infrastructure</li>
                    <li>First-class ZFS support</li>
                    <li>Robust backup tooling</li>
                    <li>A strong documentation and community ecosystem</li>
                </ul>
                <br>
                <p>Switching platforms reduced administrative overhead and enabled a more appliance-like server model.
                </p>
                <h3>Installer Compatibility Issue</h3>
                <p>During initial deployment, the graphical Proxmox installer failed during early driver initialization
                    due to a known NVIDIA compatibility issue. Installation was completed in text mode using a
                    <a href="https://pve.proxmox.com/pve-docs/pve-admin-guide.html#nomodeset_kernel_param"
                        target="_blank">documented workaround</a>.
                </p>
            </div>
            <div class="section-card">
                <h2>Storage Architecture and Data Integrity</h2>
                <p>Virtual machine storage is hosted on a ZFS RAIDZ1 pool backed by NAS-grade SSDs.</p>
                <br>
                <p>RAIDZ1 was selected to balance cost, storage efficiency, and acceptable performance for lab
                    workloads. Given the relatively small pool size, ECC memory, and regular ZFS scrubbing, this
                    tradeoff was considered appropriate for the systemâ€™s intended use.</p>
                <br>
                <p>Snapshots are used heavily for experimentation and rollback during lab work.</p>
            </div>
            <div class="section-card">
                <h2>Backup Strategy and Recovery</h2>
                <p>Rather than relying solely on Proxmox's built-in backup tooling, I deployed Proxmox Backup Server as
                    a VM
                    with a dedicated storage pool. The OS and backup disks are passed directly
                    through to the PBS VM using stable disk identifiers. This allows the entire PBS VM to be migrated to
                    bare
                    metal or another system if the host fails catastrophically.</p>
                <h3>Access Control and Least Privilege</h3>
                <p>PBS access from Proxmox VE is performed using a dedicated service account with only the
                    'DataStoreBackup' role assigned. This enforces least-privilege access.
                </p>
                <h3>Backup Characteristics</h3>
                <ul>
                    <li>Incremental, deduplicated backups</li>
                    <li>Daily snapshots with longer retention for recent data</li>
                    <li>Weekly and monthly retention for historical recovery points</li>
                    <li>Automated verification and garbage collection</li>
                </ul>
                <h3>Known Tradeoff</h3>
                <p>Virtualizing the backup server introduces a shared failure domain, which I explicitly acknowledge.
                    This tradeoff was accepted to ship a working system quickly, with plans to move PBS to dedicated
                    hardware in a future revision.</p>
            </div>
            <div class="section-card">
                <h2>Remote Management and Out-of-Band Access</h2>
                <p>Remote access was a hard requirement from the start.</p>
                <ul>
                    <li><strong>Out-of-Band management: </strong>JetKVM provides hardware-level access to UEFI,
                        bootloader, display, and ISO mounting. </li>
                    <li><strong>Secure Remote Access: </strong>Tailscale, backed by a yubikey and Passkey login,
                        provides encrypted remote access without exposing services publicly.</li>
                    <li><strong>In-band management: </strong>Proxmox web UI and SSH for routine administration.</li>
                </ul>
                <br>
                <p>Lab traffic is separated from home traffic using a dedicated VLAN.</p>
            </div>
            <div class="section-card">
                <h2>Power Management and Graceful Shutdown</h2>
                <p>As this server is hosted remotely, power loss was one of the most important failure modes to handle
                    correctly.</p>
                <br>
                <p>Using Network UPS Tools in a server-client configuration, the UPS communicates with the Proxmox host
                    and signals an orderly shutdown sequence during extended outages.</p>
                <h3>Integration Challenge</h3>
                <p>Initial NUT configuration failed due to USB device permission and service ownership issues. Community
                    documentation highlighted common pitfalls when running NUT on Proxmox VE. Aligning udev rules
                    and service permissions resolved the issue and ensured clean and reliable NUT driver initialization.
                </p>
                <h3>Outcomes:</h3>
                <ul>
                    <li>Virtual machines shut down cleanly</li>
                    <li>Backup jobs terminate safely</li>
                    <li>The host powers off last, protecting ZFS integrity</li>
                </ul>
                <br>
                <p>Shutdown timing and battery thresholds were tuned based on observed runtime rather than defaults.</p>
                <br>
                <p>UPS status is also surfaced through HomeKit via a Homebridge container, providing at-a-glance
                    visibility into power state.</p>
            </div>
            <div class="section-card">
                <h2>Validation and Testing</h2>
                <p>To ensure reliability beyond initial configuration, key failure scenarios were validated:</p>
                <ul>
                    <li>UPS-triggered shutdowns</li>
                    <li>Verified backup restore functionality from Proxmox Backup Server</li>
                    <li>Tested remote recovery using JetKVM during host reboots</li>
                </ul>
            </div>
            <div class="section-card">
                <h2>What I Learned</h2>
                <p>This project reinforced that technical success often comes from choosing the path with the least
                    long-term friction, not the one that looks best on paper or easiest to get running.</p>
                <br>
                <p>Pivoting away from an all-Windows stack improved reliability and manageability, even though it
                    required deeper Linux administration and research. Documentation, community knowledge, and
                    troubleshooting skills were invaluable.</p>
                <br>
                <p>Most importantly, I learned that failure is a natural part of the learning process, and that it's
                    important to be willing to pivot and try new things.</p>
            </div>
            <div class="section-card">
                <h2>What I'd Do Differently</h2>
                <ul>
                    <li><strong>Native IPMI: </strong>A motherboard with built-in BMC would simplify telemetry and
                        reduce reliance on external hardware.</li>
                    <li><strong>Simplified Remote Network Access: </strong>A dedicated Tailscale subnet router would
                        reduce per-host configuration overhead.</li>
                    <li><strong>Dedicated Backup Server: </strong>My one true regret. Separating PBS onto a dedicated,
                        low-power system would remove the shared failure domain.</li>
                </ul>
            </div>
            <div class="section-card">
                <h2>Future Roadmap</h2>
                <ul>
                    <li>Active Directory lab environment</li>
                    <li>Isolated cybersecurity and DFIR sandboxes</li>
                    <li>Potential local AI workloads as hardware prices improve</li>
                </ul>
            </div>
            <div class="section-card">
                <h2>Conclusion</h2>
                <p>This project represents a transition from ad-hoc experimentation to deliberate system design.</p>
                <br>
                <p>By repurposing hardware, embracing open-source tooling, and prioritizing recoverability, I built a
                    platform that mirrors many of the operational realities of enterprise infrastructure at a personal
                    scale.</p>
                <br>
                <p>This system now serves as a stable foundation for coursework, security labs, and future
                    enterprise-focused experimentation.</p>
            </div>
            <div class="section-card">
                <h2>Demonstrated Technologies & Skills</h2>
                <div>
                    <span class="tech-tag">Virtualization (Proxmox VE)</span>
                    <span class="tech-tag">System Architecture</span>
                    <span class="tech-tag">Backup & Disaster Recovery</span>
                    <span class="tech-tag">Remote & Out-of-Band Management</span>
                    <span class="tech-tag">Power & Availability Engineering</span>
                    <span class="tech-tag">Linux Systems Administration</span>
                    <span class="tech-tag">Security Fundamentals (RBAC, Least Privilege)</span>
                    <span class="tech-tag">Operational Validation & Testing</span>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <p>&copy; 2026 Joseph Broda</p>
    </footer>
</body>

</html>